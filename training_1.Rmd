---
title: "Training 1: Documentation"
output: 
  html_document:
    css: "css/custom.css"
    toc: TRUE
    toc_float: TRUE
---

<br>

:::presentation
<br>
You can view slides from this talk [here](https://cghlewis.github.io/mpsi-training1/).
:::
  
---

## Data Management Plan

---

To increase transparency and to advance scientific inquiry in education research, the Institute of Education Science ([IES](https://ies.ed.gov/funding/datasharing_implementation.asp)) has a policy on providing access to your data at the conclusion of projects funded through their research centers (*Exploration* or *Initial Efficacy and Follow-Up* grants). The plan for sharing data must be laid out in a data management plan (DMP) which is included as an appendix in your grant application. Among other things, the DMP must include the following:  

‚úîÔ∏è Type of data to be shared  
‚úîÔ∏è Procedures for managing and maintaining confidentiality  
‚úîÔ∏è Roles and responsibilities of project staff in the management of research data  
‚úîÔ∏è Expected schedule for data sharing (no later than the publication of findings and at least for 10 years)  
‚úîÔ∏è Format of the final dataset  
‚úîÔ∏è Documentation to be provided  
‚úîÔ∏è Method of data sharing (Share the data yourself, use a data archive, combination of both)  
‚úîÔ∏è Whether or not a data sharing agreement specifies the conditions under which the data will be shared  
‚úîÔ∏è Any circumstances that prevent any data from being shared  
‚úîÔ∏è Most recent rule: A plan for [pre-registering](https://ies.ed.gov/seer/preregistration.asp) your study within the first year of the project  

This training series is not specific only to IES grants. Almost all funders that education researchers may work with require a DMP that also includes a data sharing or archival plan (ex: [NIJ](https://nij.ojp.gov/funding/research-development-and-evaluation-grant-award-requirements#vj8jg) and [NIH](https://grants.nih.gov/grants/policy/data_sharing/data_sharing_guidance.htm#doc)). Furthermore, this training will not cover the details of writing up a DMP but rather how to implement a DMP. We will cover topics such as: How do you document data; How do you track it; How do you manage it; How do you keep it secure; and How do you share it. This specific training module will cover documentation.

Before we dive in, as IES is the most prolific funder of RCTs in education, if you are someone who will be writing IES DMPs in the future or want to know more about them, here are some excellent resources:

  üìë [University of Virginia IES DMP   template](https://data.library.virginia.edu/files/IES-Data-Management-Plan-Template-2018.docx)  
  üìë [IES DMP tool](https://dmptool.org/template_export/1895.pdf)  
  üìë [IES most recent Request for Applications](https://ies.ed.gov/funding/pdf/2021_84305A.pdf)  
  [Foundational Practices of Research Data Management](https://riojournal.com/article/56508/instance/5569679/)

---

## Documentation

---

Data Documentation is not only required by funders including IES, it is essential for data management. Data documentation should cover the who, what, where, when, and why of your project and your data ([OSF: NIH](https://osf.io/48ufs/)).


```{r, echo=FALSE, fig.align="center", out.width='70%'}

knitr::include_graphics("img/document.png")

```

Documentation also allows you to: 

* Track decisions/changes made throughout the life cycle of the project  
* Make our decisions/analyses replicable   
* Clean our data with fidelity  
* Ensure others use and interpret our data accurately  
* Discover errors in our data  
* Allows others to find our archived data through metadata

Documentation can take many forms. [IES](https://ies.ed.gov/funding/datasharing_implementation.asp) states that they expect documentation to "be a comprehensive and stand-alone document that includes all the information necessary to replicate the analysis performed by the original research team" and should include:  

* a summary of the purpose of the data collection  
* methodology and procedures used to collect the data  
* timing of the data collection  
* details of the data codes, definition of variables, variable field locations, and frequencies  

However, a) IES provides no template for how documentation is laid out OR what tools to use to create/share this information and b) outside of IES required documentation for data sharing, there may be additional documentation your team may want to keep to help manage internal processes. 

This training will cover types of documentation you may want to keep. Many of these documents have overlapping information and some terms may be used interchangeably in the field (ex: Data dictionary and codebook or README and metadata). Also, some of the documents are important to start day one of the project, while others are more important to create at the end of the project when you are ready to share data. The point of this next section isn't to implement all of these documents, but rather to consider which documents capture the information you need to have a successful project. It may just be a protocol and a data dictionary. Or it may be a protocol, a data dictionary and a README for each data file. Or it may be all of these documents! It depends on things such as the scale of your project and how you plan to share the data at the end of your project.

Additional reading on the importance of data documentation can be found here:

  üìë [University of Helsinki](https://www.helsinki.fi/en/research/guide-for-data-documentation)  
  üìë [Washington University in St. Louis](https://libguides.wustl.edu/c.php?g=47355&p=303435)
  

<br>

### üìì Protocol

A document/s to record all your procedures changes made to those procedures throughout the grant.

At some point someone will ask you how or why you did something the way you did. This is the document that will save you because it is very likely that you will not remember. This document also helps standardize your procedures. You can and should refer back to this document during a longitudinal study to ensure you continue to implement procedures in the same way. 

I highly recommend always adding protocol as part of any data documentation plan. You can make separate protocol documents per procedure or keep them all in one standalone document with a table of contents. Most likely your protocol will not be shared outside of your team, but the information in your protocol can be used to inform other documentation such as READMEs or codebooks. Your protocol can live in any format that works for you (ex: .docx, .md, .txt). It is a living document that will be continually updated so use a format that makes sense for you. 

Protocol should cover procedures/decisions for the following:

* Participant recruitment
* Consent and assent
* Participant selection
* Randomization and blinding/unblinding
* Data collection
* Staff training
* Data entry, data retrieval, data scoring
* Payments/incentives
* Intervention implementation

Each protocol should *begin* with the following:

* Title
* Date the protocol was made
* Who made the protocol
* Any rationale behind the protocol
* Any related documents or research behind the protocol

For any *changes* to the protocol after the project has begun, add the following below the original protocol   section: 

* Revision Date  
* Who decided on the revision  
* Any rationale behind the revision  
* Any related documents or research behind the revision
  
It is also good practice to add a copy of your data collection instruments to your protocol (Surveys, Interview Questions, etc.), as well as changes/versions of those and explanations for the changes.
  
  üìë These [slides](https://figshare.com/articles/Data_Management_and_Data_Management_Plans/7890827) from Jessica Logan, Ph.D. have nice protocol examples.

<br>

### üìì Style Guide

A [style guide](https://en.wikipedia.org/wiki/Style_guide) is a set of standards around data management rules. It improves consistency within and across files and projects. This document includes conventions for procedures such as variable naming, value coding, file naming, versioning, and file structure. Consider having a style guide specific to every project (for variable naming, value coding) as well as a general style guide that applies to all projects (for file structure and file naming). This guide can be be kept in any form such as .txt or .pdf and should be housed where it can easily be accessed by all team members (Ex: a style guide that applies across projects may be kept on a wiki and a project-specific style guide may be kept as a protocol or README in a project folder). See [Training2] for best practices in style guides.

Example [style guide](https://github.com/bartvandebiezen/file-name-conventions/blob/master/README.md).

<br>

### üìì Wiki

I am including a wiki in this list even though I think it is an unconventional documentation tool in education research. A wiki can help your team internally document, organize, and navigate reference material. If you use platforms such as Microsoft SharePoint, a wiki can be a great place to store information that your team frequently refers to. SharePoint describes a wiki as "a site that is designed for groups of people to quickly capture and share ideas by creating simple pages and linking them together." It is a page that anyone on your team can add/edit content (a short blurb about the document) and then can link to those referenced documents for easy access. 

Wikis can be made for specific research projects and are great for referring to frequently requested information (ex: recruitment information or participant payment details). You can also make a general research team wiki that includes information that either applies to your entire team (ex: Team meeting notes or employee manual) or information that is relevant across all projects (ex: Style guide). 

  üìë You can make wikis with many other tools, but you can read more about Sharepoint's wiki [here](https://support.microsoft.com/en-us/office/create-and-edit-a-wiki-dc64f9c2-d1a2-44b5-ac59-b9d535551a32).

  üìë There is a great episode of the [Education Data Chat](https://www.buzzsprout.com/1074286/5185513-episode-5-organizing-shared-network-drives-tips-and-tricks) podcast where they discuss wikis as well.

<br>

### üìì Data Dictionary

A data dictionary is another essential data management document and I highly suggest you start this before your ever collect a piece of data. This will help you set up successful back-end coding as you create data collection tools and it will serve as a reference document you can use as questions come up about the data. It is also a document that can be shared with others to help them better understand your data. A data dictionary is usually in rectangular format, for example an excel spreadsheet, google sheet or something similar that has rows and columns. It includes all information relevant to every variable in your data. 

  üìë Here is a nice [example](https://media.screensteps.com/image_assets/assets/001/878/671/original/8fbcc565-602f-4ac6-b828-2eaaeb67578b.png) of a very simple data dictionary.
  
Your dictionary **should** capture information such as:

* Variable Name  
* Variable Label  
* Exact question text  
* Associated scale (if the item belongs to a scale)
* Value range or value codes ([1-99] or 0=No, 1=Yes)  
* Measurement unit (numeric, string, date, etc.)  
* Missing data codes
* Variable universe (Who gets this question? Is it skipped for some people?) 
* What time periods/years does this variable exist 
* Reverse coding
* Calculations (composite variables, scores)
* Notes (such as versions/changes to this variable)  
* Group variables by measure (Student survey, teacher rating of student, etc.) and consider including the order number of the question on the measure

You may consider having separate data dictionaries per expected final dataset. So for example, if you plan to have a final teacher dataset and a final student dataset, you may want to have a student dictionary and a teacher dictionary rather than having only one data dictionary and marking teacher or student in the universe column.


As I mentioned earlier, it is important to start a data dictionary **before** ever collecting data. You should be able to begin filling in information for the data you have control over. For example, if you are making and administering the survey, you should be able to know what variables will be in that data, how you want to name them, and how they should be coded. However, you may not know this information for other variables until after data collection. For example, if you are administering a 3rd party assessment, you may not know what the exported data is going to look like until after you have collected the data and it has been scored. Just remember, this is a living document that will updated continually throughout the life of the project. 

**In order to start a data dictionary, you should gather the following information**:

* What measures are we collecting? (Ex: Student Assessment, Teacher Observation, Teacher Survey, Principal Interview)
* What are the items/scales included?
* What is your variable naming protocol? (See variable naming)
* Do your items/scales have pre-determined value coding rules or can we assign our own? What is our value coding protocol? (See value coding)
    
It's good to also consider starting to document the variables you aren't collecting but you are **assigning** or **calculating**. 

* Variables you are assigning:
    + To maintain anonymity and to track participants and merge data over time, you will be   assigning identifiers. 
      + Participant ID (Ex: *studentid*)
      + Location ID (Ex: *schoolid*, *districtid*)
    + Additionally, if you are collecting longitudinal data, you most likely need to create   variables that indicate time. Consider both the cohort (if applicable) as well as the round of data collection. (Ex: *cohort*, *year*, *time*)   
    
* Variables you are calculating:
    + If you are collecting survey, assessment, or observation data, you will likely have composite variables and/or calculated scores in your data. You can start to add these variables and their calculations to your data dictionary.
      + Ex: *scale_sum*, *scale_mean*, *math_ss* (for scaled score)
    
:::question
**Quick thought: Accounting for Time**

When deciding how to account for time, consider how you plan to merge your data across time.

   a) Merge wide (each case is one row and all associated time points are in that row), then append time to your variables (prefix or suffix) (Ex: *t1_scale1*, *t2_scale1*) <br>
   b) Merge long (a case occurs in multiple rows for each time period collected), then time can be its own variable (Ex: *time* = 1 or 2)

Additionally, depending on your study design, you may have multiple ways of indicating time (*cohort*, *year*, *time*, etc.). Plan out which of these will be unique variables and which, if any, will be appended to your variables.
:::

Additional reading on data dictionaries can be found here:

  üìë [OSF](https://help.osf.io/hc/en-us/articles/360019739054-How-to-Make-a-Data-Dictionary)  
  üìë [USDA](https://data.nal.usda.gov/data-dictionary-purpose)  
  üìë [University of Helsinki](https://www.helsinki.fi/en/research/guide-for-data-documentation#section-72708)  
  üìë [Karl Broman](http://kbroman.org/dataorg/pages/dictionary.html)

<br>

### üìì Codebook

Like a data dictionary, a codebook also captures variable information, as well as project level information. It should capture all a user would need to know in order to understand and correctly interpret your data. It is typically part of the metadata that is added to a data archive, or is given to those who request your data after the project is complete. It usually takes the form of a plain text file (.txt), pdf (.pdf), or extensible markup language (.xml), rather than a proprietary form such as .docx. If you embed metadata into your data, such as variable labels, value labels and missing values, several statistical programs (including R, SPSS, SAS and Stata) will export codebooks for your final datasets. You can make one large codebook that includes all data for your project, or make separate codebooks by dataset (ex: Teacher level dataset codebook), or even by measure (ex: Student survey codebook). 

  üìë Here is a nice example of a [codebook](https://ddialliance.org/sites/default/files/06551.pdf).

Your codebook should captures things such as:

* Study title  
* Names of Investigators  
* Table of contents  
* Purpose and format of the codebook  
* Variable level details  
  + Variable name  
  + Variable label  
  + Question text  
  + Measurement unit
  + Coded values or value range (1,2,3,4)  
  + Value labels (*excellent*, *good*, *fair*, *poor*)  
  + Summary statistics  
  + Missing data values  
  + Skip patterns  
  + Notes  
  + If data is in text format, indicate position of each variable
* Computations
* Imputations

Other optional content for a codebook: 

* Citations for scales used
* Question ordering
* Data collection instruments 
* Consent agreements
* Methodological details  
* Flowchart of data collection instruments/screeners 
* Study timeline

*Last, I have found that sometimes the term data dictionary is synonymous with codebook. I don't think the name matters, but for the sake of this training I am specifying a data dictionary as the document in tabular form with only variable level information and a codebook as a document usually in text format. You can keep either or both types of documentation. I think the data dictionary as laid out in this training is the easier document to update throughout the life of the project and assists with project implementation, while I view the codebook as a document to summarize the final datasets at the end of the project (and to be included with data archives/requests).*

Further information on codebooks can be found here:

  üìë [ICPSR](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html)  
  üìë [SAMHDA](https://www.datafiles.samhsa.gov/faq/what-codebook-nid3440)  
  üìë [Kai Horstmann](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjb8-TwpMPsAhUM2qwKHXFdB1cQFjAKegQIBBAC&url=https%3A%2F%2Fosf.io%2F72hrh%2F%3Faction%3Ddownload%26version%3D2&usg=AOvVaw340qQlC9mDMzc444K_dkhT)

<br>

### üìì README

A README is a plain text (.txt), markdown (.md), pdf (.pdf), or extensible markup language (.xml), standalone file that explains your files. README files are most known for their use in programming, but have become more prevalent in research. It is recommended to make one README file per dataset. This standalone document will accompany each dataset. It should be named so it is easily associated with the dataset and it should be housed in the same folder the data is in. 

A README should capture data such as:  

* General information (Title of dataset, contact information, key dates, funding)
* File information (Description, format, creation date, versions)
* Access information (Licenses, recommended citation, associated publications)
* Methodology (Consent rates, data collection methods, data processing methods, software used, quality-assurance procedures)
* Optional: Your codebook can also be part of your README, rather than a separate document


Excellent examples, templates, and further details can be found here:

  üìë [README with Codebook included: University of Munster](https://osf.io/y2gjr/)  
  üìë [README template: Oregon State](https://guides.library.oregonstate.edu/ld.php?content_id=45294345)  
  üìë [README template: Cornell](https://cornell.app.box.com/v/ReadmeTemplate)      
  üìë [README template: OSF](https://osf.io/sj8xv/)  
  üìë [README recommended content: UCI](https://guides.lib.uci.edu/datamanagement/readme)  
  üìë [README recommended content: Cornell](https://data.research.cornell.edu/content/readme)    
  üìë [README recommended content: IHEID](https://libguides.graduateinstitute.ch/rdm/readme)   

<br>

### üìì Metadata

Metadata is *data about data*. They are structured data that "provide information about the dataset to help people find, understand, and use your data" ([IES](https://ies.ed.gov/funding/datasharing_faq.asp)). Most of the time when you hear the term metadata, it will be referring to what I am going to call project-level metadata. Project-level metadata is a type of descriptive metadata that aids in finding your project through internet searches. It is like the "label of the dataset". At minimum the information captured should include who created the data, when the data were created, a title and description of the data, and a unique identifier for the data. This is the type of data we discussed capturing earlier within the *General*, *File*, and *Access* portion of our README files. 

Common elements of project-level metadata include:

```{r, echo=FALSE, fig.align="center", out.width='80%'}

knitr::include_graphics("img/metadata.png")

```
Source: [University of Portland](https://libguides.up.edu/datamanagement/documentation)

Metadata can also also be categorized further into data-level and variable-level:

* Data-level metadata, which might be considered administrative metadata, focuses more on the details of each dataset such as instruments used to collect the data and software used to process the data. Lucikly, creating this metadata requires no extra effort because this is the exact information captured in sections like *Methodology* in your READMEs above.

* Variable-level metadata, which might be considered structural metadata, is data about the variables in your dataset. This can be both incorporated into your data, by adding attributes like variable and value labels, as well as accounted for externally, and it is includes all the information discussed earlier in your codebook and/or data dictionary. If your data is qualitative in nature (such as documents, photos, videos, or sound files), metadata such as date/time/location, tags/keywords, or measurement information can sometimes be embedded directly into your files ([IHEID](https://libguides.graduateinstitute.ch/rdm/qualitative)). 

The most notable thing about metadata is that many fields follow metadata standards that aid in the "retrieving and indexing" of your data ([University of Arizona](https://data.library.arizona.edu/best-practices/data-documentation-readme-metadata)). Standards include things such as agreed upon formats, vocabularies, and structure. For example social science often follows the Data Documentation Initiative ([DDI](https://ddialliance.org/)) standards. However, ([IES](https://ies.ed.gov/funding/datasharing_faq.asp)) states that education has no standards and that instead "researchers should document everything and strive to make notation as interpretable as possible". Metadata can either be embedded within data or included in a separate file such as a README.txt or .pdf file.

If you plan to archive your data, the data repository you plan to use may have standards in place that you will need to follow. Consult your repository's website for detailed information on what is required for archival. Many universities have archives that are available for use and IES also has recommended data repositories that can be found [here](https://ies.ed.gov/funding/datasharing_faq.asp).


Additional metadata information can be found here:  

  üìë [Oregon State](https://guides.library.oregonstate.edu/research-data-services/data-management-metadata#:~:text=Project%2Dlevel%20metadata%20describes%20the,Dataset%20title)  
  üìë [UCI](https://guides.lib.uci.edu/datamanagement/describe)    
  üìë [NCSU](https://www.lib.ncsu.edu/do/data-management/metadata)  
  üìë [ICPSR](https://www.icpsr.umich.edu/web/pages/datamanagement/dmp/plan.html)   
  üìë [DDI](https://ddialliance.org/training/getting-started/data-catalog)  
  üìë [ADS](https://guides.archaeologydataservice.ac.uk/g2gp/CreateData_1-2#ref-CreateData_1-2-9)  
  üìë [University of Helsinki](https://www.helsinki.fi/en/research/guide-for-data-documentation)  
  üìë [London School of Economics and Political Science](https://www.lse.ac.uk/library/research-support/research-data-management/metadata-and-documentation)  
  üìë [Washington University in St. Louis](https://libguides.wustl.edu/c.php?g=47355&p=305263)  
  üìë [Data Management for Researchers](https://pelagicpublishing.com/products/data-management-for-researchers-briney)  

<br>

### üìì Miscellaneous

Last you should consider documenting **all** your data processes and files to allow for reproducibility. This may fit into one of the above mentioned documents, or it may be a separate text or markdown files (such as a README), a syntax file, or even an excel file. Keep this document in the folder related to the content and name it accordingly. 
Consider tracking things such as:

* File origination information (Ex: `README_district-file-versions.txt`):
  * If you use version control, this is easy to track through commit messages or commenting. However, if you do not use software that has version control, consider keeping a document that tracks differences between files.
    + `district-data_2020-10-02.xlsx`: File received from District A containing 2019-20 test scores
    + `district-data_2020-10-03.xlsx`: File received from District A with 5 additional students added after `district-data_2020-10-02.xlsx` was received
    + `district-data_2020-10-04.xlsx`: File received form District A with test scores updated for 30 students because errors were found in `district-data_2020-10-03.xlsx`  
<br>
  
* Data cleaning plan (Ex: `district_data-cleaning-plan.md`)

  * This will be discussed more in later trainings, but a written analysis plan can be helpful to communicate across your team, as a reminder to your future self, and can also be added to your documentation or a technical appendix for a report. This plan can be in a standalone text file, markdown file, or as comments in a syntax file and should include, in *prose*, the steps you plan to take to take your data from raw form to final analysis/clean form. This allows anyone to follow your data cleaning process, even if they don't understand your actual code (ex: R or Stata code)  
<br>
  
* Actual files used in data cleaning process (Ex: `README_district_data-cleaning-process.txt`). 
  * Data cleaning plan: `district_cleaning-plan.md`
  * Syntax: `district_cleaning-syntax.R`  
  * Input: `district-data_raw_2020_09_08.xlsx`  
  * Output: `district-data_clean_2020_10_09.csv`   
<br>

* A setup file for steps to create tables and reports (Ex: `00_setup.md`):
  * Step 1: Run the file `01_clean-the-data.R` to clean the data 
  * Step 2: Run the file `02_check-errors.R` to check for errors
  * Step 3: Run the file `03_run-report.R` to create the report  


<br>
  
### üìì  Conclusion

The main takeaway is that it is important to document everything. Not only is it required by funders and by data repositories, as well as necessary for those submitting data requests, but it is crucial to the integrity and reproducibility of your project. While you should try to standardize your documentation as much as possible, don't get stuck in the details of the format or the structure. Instead focus on getting the content, rules, and procedures down before you forget.

Although, *publishing data* will come in a later module, I did want to wrap up by saying that when it comes time to publish your data (whether in a repository or data sharing through your own request system), plan to include the following documentation with your final datasets:

* README file (for each dataset) - including the project-level metadata
* Data dictionaries and/or codebooks

üìë If you want to see an amazingly thorough (and very long) example of data documentation, explore the [Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 User's Manual](https://nces.ed.gov/pubs2015/2015074.pdf) from IES.
