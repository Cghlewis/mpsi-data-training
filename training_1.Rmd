---
title: "Training 1: Documentation"
author: "Crystal Lewis  "  
output: 
  html_document:
    css: "css/custom.css"
    toc: TRUE
    toc_float: TRUE
---

<br>

:::presentation
<br>
You can view slides from this talk [here](https://cghlewis.github.io/mpsi-training1/).
:::
  
---

## Data Management Plan

---

To increase transparency and to advance scientific inquiry in education research, the Institute of Education Science ([IES](https://ies.ed.gov/funding/datasharing_implementation.asp)) has a policy on providing access to your data at the conclusion of projects funded through their research centers (*Exploration* or *Initial Efficacy and Follow-Up* grants). The plan for sharing data must be laid out in a data management plan (DMP) which is included as an appendix in your grant application. Among other things, the DMP must include the following:  

‚úîÔ∏è Type of data to be shared  
‚úîÔ∏è Procedures for managing and maintaining confidentiality  
‚úîÔ∏è Roles and responsibilities of project staff in the management of research data  
‚úîÔ∏è Expected schedule for data sharing (no later than the publication of findings and at least for 10 years)  
‚úîÔ∏è Format of the final dataset  
‚úîÔ∏è Documentation to be provided  
‚úîÔ∏è Method of data sharing (Share the data yourself, use a data archive, combination of both)  
‚úîÔ∏è Whether or not a data sharing agreement specifies the conditions under which the data will be shared  
‚úîÔ∏è Any circumstances that prevent any data from being shared  
‚úîÔ∏è Most recent rule: A plan for [pre-registering](https://ies.ed.gov/seer/preregistration.asp) your study within the first year of the project  

This training is not specific only to IES grants. Almost all funders that education researchers may work with require a DMP that also includes a data sharing or archival plan (ex: [NIJ](https://nij.ojp.gov/funding/research-development-and-evaluation-grant-award-requirements#vj8jg) and [NIH](https://grants.nih.gov/grants/policy/data_sharing/data_sharing_guidance.htm#doc)). Furthermore, this training will not cover the details of writing up a DMP but rather how to implement a DMP. We will cover topics such as: How do we document data; How do we track it; How do we manage it; How do we keep it secure; and How do we share it. This specific training module will cover documentation.

Before we dive in, as IES is the most prolific funder of RCTs in education, if you are someone who will be writing IES DMPs in the future or want to know more about them, here are some excellent resources:

  üìë [University of Virginia IES DMP   template](https://data.library.virginia.edu/files/IES-Data-Management-Plan-Template-2018.docx)  
  üìë [IES DMP tool](https://dmptool.org/template_export/1895.pdf)  
  üìë [IES most recent Request for Applications](https://ies.ed.gov/funding/pdf/2021_84305A.pdf)
  [Foundational Practices of Research Data Management](https://riojournal.com/article/56508/instance/5569679/)

---

## Documentation

---

Data Documentation is not only required by funders including IES, it is essential for data management. 
It also allows us to: 

* Track decisions/changes made throughout the life cycle of the project  
* Make our decisions/analyses replicable   
* Clean our data with fidelity  
* Ensure others use and interpret our data accurately  
* Discover errors in our data  
* Allows others to find our archived data through metadata

Documentation can take many forms. [IES](https://ies.ed.gov/funding/datasharing_implementation.asp) states that they expect documentation to "be a comprehensive and stand-alone document that includes all the information necessary to replicate the analysis performed by the original research team" and should include:  

* a summary of the purpose of the data collection  
* methodology and procedures used to collect the data  
* timing of the data collection  
* details of the data codes, definition of variables, variable field locations, and frequencies  

However, a) IES provides no template for how documentation is laid out OR what tools to use to create/share this information and b) outside of IES required documentation for data sharing, there may be additional documentation your team may want to keep to help manage internal processes. 

We will go over types of documentation you may want to consider keeping. Many of these documents have overlapping information and some terms may be used interchangeably in the field (ex: Data Dictionary and Codebook or README and metadata). Also, some of the documents are important to start day one of the project, while others are more important to create at the end of the project when you are ready to share data. The point of this next section isn't to implement all of these documents, but rather to consider which documents capture the information you need to have a successful project. It may just be a protocol and a data dictionary. Or it may be a protocol, a data dictionary and a README for each data file. Or it may be all of these documents! It depends on things such as the scale of your project and how you plan to share the data at the end of your project.

Additional reading on the importance of data documentation can be found here:

  üìë [University of Helsinki](https://www.helsinki.fi/en/research/guide-for-data-documentation)  
  üìë [Washington University in St. Louis](https://libguides.wustl.edu/c.php?g=47355&p=303435)
  

<br>

### üìì Protocol

A document/s to record all your procedures changes made to those procedures throughout the grant.

At some point someone will ask you how or why you did something the way you did. This is the document that will save you because it is very likely that you will not remember. I highly recommend always adding protocol as part of any data documentation plan. You can make separate protocol documents per procedure or keep them all in one standalone document with a table of contents. Most likely your protocol will not be shared outside of your team, but the information in your protocol can be used to inform other documentation such as READMEs or Codebooks. Your protocol can live in any format that works for you (ex: .docx, .md, .txt). It is a living document that will be continually updated so use a format that makes sense for you. 

Protocol should cover procedures/decisions for the following:

* Participant recruitment
* Consent and assent
* Participant selection
* Randomization and blinding/unblinding
* Data collection
* Staff training
* Data entry, data retrieval, data scoring
* Payments/Incentives
* Intervention implementation

Each protocol should begin with the following:

* Title
* Date the protocol was made
* Who made the protocol
* Any rationale behind the protocol
* Any related documents or research behind this protocol

* For any changes to the protocol after the project has begun add the following below the original protocol   section:  
  + Revision Date  
  + Who decided on the revision  
  + Any rationale behind the revision  
  + Any related documents or research behind the revision
  
It is also good practice to add a copy of your data collection instruments to your protocol (Surveys, Interview Questions, etc.), as well as changes/versions of those and explanations for the changes.
  
  üìë These [slides](https://figshare.com/articles/Data_Management_and_Data_Management_Plans/7890827) from Jessica Logan, Ph.D. have nice protocol examples.

<br>

### üìì Wiki

I am including a wiki in this list even though I think it is an unconventional documentation tool in education research. A wiki can help your team internally document, organize, and navigate frequently used information. If you use platforms such as Microsoft SharePoint, a wiki can be a great place to store high level information that your team frequently refers to. SharePoint describes a wiki as "a site that is designed for groups of people to quickly capture and share ideas by creating simple pages and linking them together." It is a page that anyone on your team can add/edit content and then can link to those referenced documents for easy access. 

Wikis can be made for specific research projects and are great for referring to frequently requested information (ex: Recruitment information or participant payment details). You can also make a general research team wiki that links to information that either applies to your entire team (ex: Team meeting notes or employee manual) or information that is relevant across all projects (ex: Data management rules such as file naming conventions, file structure, or versioning rules).

  üìë You can make wikis with many other tools, but you can read more about Sharepoint's wiki [here](https://support.microsoft.com/en-us/office/create-and-edit-a-wiki-dc64f9c2-d1a2-44b5-ac59-b9d535551a32).

  üìë There is a great episode of the [Education Data Chat](https://www.buzzsprout.com/1074286/5185513-episode-5-organizing-shared-network-drives-tips-and-tricks) podcast where they discuss wikis as well.

<br>

### üìì Data Dictionary

A data dictionary is another essential document to keep and I highly suggest you start this before your ever collect a piece of data. This will help you set up successful back end coding as you create data collection tools as well as being a document you can refer back to as questions come up about the data. It is also a document that can be shared with others to better understand your data. This document is usually in rectangular form, for example an excel spreadsheet, google sheet or something similar that has rows and columns. It includes all information relevant to every variable in your data. 

  üìë Here is a nice [example](https://media.screensteps.com/image_assets/assets/001/878/671/original/8fbcc565-602f-4ac6-b828-2eaaeb67578b.png) of a very simple data dictionary.

**In order to start a data dictionary, you need to know the following**:

* What data are we collecting? (Ex: Student Assessment, Teacher Observation, Teacher Survey, Principal Interview)
* What are the questions/measures included?
* What is your variable naming protocol?
  + Have you used the question/measure before? If yes, keep the variable name the same across projects.
  + A typical variable naming convention for survey questions is AbbreviatedScaleName# (Ex: toca1, toca2, toca3)
    + If the question wording or response options change during the project, make a rule for versioning
      + Ex: Add "v2" on revised questions (toca1v2)
  + Be consistent with capitalization (ex: always using lowercase)
  + No spaces or special characters in variable names
  + Be consistent with delimiters (ex: use snakecase or camel case)
    + Camel case (TocaConcentration)
    + Snake case (toca_concentration)
  + Make sure all variable names are unique
  + Names should be meaningful but concise (Ex: SPSS cannot handle variable names over 64 characters)
* Do your measures have pre-determined value coding rules or can we assign our own?
* Are we collecting data over time? What is our Cohort/Year/Time variable protocol?
  + If you plan to collect multiple time points of data and merge data in a wide format, you will need a time prefix or suffix on your variables (ex: toca1_T1 or toca1_Fall)
  + If you plan to track a cohort over multiple years in wide format, you may need a second prefix or suffix (ex: toca1_Year1_T1)
  + If you plan to merge data in long format, time can be their own variables (Ex: Time, Year, Cohort)
  
Your dictionary **should** capture information such as:

* Variable Name  
* Variable Label  
* Exact question text  
* Value range or value codes  
* Measurement unit (Ex: numeric, string, date)  
* How is missing data coded  
* Variable universe (Who gets this question?, Is it skipped for some people?) 
* What time periods/years does this variable exist 
* Calculations
* Notes (such as versions/changes to this variable)  
* Summary statistics 
* Also consider grouping variables by measure and by form (Student Survey, Teacher Rating of Student)

You may even consider having separate data dictionaries per expected final dataset. So for example, if you plan to have a final teacher dataset and a final student dataset, you may want to have a student dictionary and a teacher dictionary rather than stating teacher or student in the universe column.

Last, as I mentioned before, it is important to start this document BEFORE ever collecting data. **But that doesn't mean that you will be able to fill in all the cells before collecting the data.** Fill in what you can. You should be able to fill in the data you have control over. For example, if you are making and administering the survey, you should be able to know what variables will be in that data, how you want to name them, and how they should be coded. However, you may not know this information for other data until after data collection. For example, if you are administering a 3rd party assessment, you may not know what the downloaded data is going to look like until after you have collected the data and it has been scored. Also, other fields such as notes, time periods, and maybe even summary statistics are those that will be continually updated throughout the project.

Additional reading on data dictionaries can be found here:

  üìë [OSF](https://help.osf.io/hc/en-us/articles/360019739054-How-to-Make-a-Data-Dictionary)  
  üìë [USDA](https://data.nal.usda.gov/data-dictionary-purpose)  
  üìë [University of Helsinki](https://www.helsinki.fi/en/research/guide-for-data-documentation#section-72708)  

<br>

### üìì Codebook

Like a data dictionary, a codebook also captures variable information, as well as project level information. It should capture all a user would need to know in order to understand and correctly interpret your data. It is typically part of the metadata that is added to a data archive, or is given to those who request your data after the project is complete. It usually takes the form of a plain text file (.txt), pdf (.pdf), or extensible markup language (.xml), rather than a proprietary form such as .docx. If you embed metadata into your data, such as variable labels, value labels and missing values, several statistical programs (including R, SPSS, SAS and Stata) will export codebooks for your final datasets. 

  üìë Here is a nice example of a [codebook](https://ddialliance.org/sites/default/files/06551.pdf).

Your codebook should captures things such as:

* Study title  
* Names of Investigators  
* Table of contents  
* Purpose and format of the codebook  
* Variable level details  
  + Variable name  
  + Variable label  
  + Question text  
  + Coded values (1,2,3,4)  
  + Value labels (Excellent, Good, Fair, Poor)  
  + Summary statistics  
  + Missing data values  
  + Skip patterns  
  + Notes  
  + If data is in text format, indicate position of each variable
* Computations
* Imputations

Other optional content for a codebook: 

* Data collection instruments 
* Consent agreements
* Methodological details  
* Flowchart of data collection instruments  

*Last, I have found that sometimes the term data dictionary is synonymous with codebook. I don't think the name matters, but for the sake of this training I am specifying a data dictionary as the document in tabular form with only variable level information and a codebook as a document usually in text format. You can keep either or both types of documentation. I think the data dictionary as laid out in this training is the easier document to update throughout the life of the project and assists with project implementation, while I view the codebook as a document to summarize the final datasets at the end of the project (and to be included with data archives/requests).*

Further information on codebooks can be found here:

  üìë [ICPSR](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html)  
  üìë [SAMHDA](https://www.datafiles.samhsa.gov/faq/what-codebook-nid3440)  

<br>

### üìì README

A README is a plain text (.txt), markdown (.md), pdf (.pdf), or extensible markup language (.xml), standalone file that explains your files. README files are most known for their use in programming, but have become more prevalent in research. It is recommended to make one README file per dataset. This standalone document will accompany each dataset. It should be named so it is easily associated with the dataset and it should be housed in the same folder the data is in. 

A README should capture data such as:  

* General information (Title of dataset, contact information, key dates, funding)
* File information (Description, format, creation date, versions)
* Access information (Licenses, recommended citation, associated publications)
* Methodology (Data collection methods, data processing methods, software used, quality-assurance procedures)
* Optional: Your codebook can also be part of your README, rather than a separate document


Excellent templates and further details can be found here:

  üìë [README template: Oregon State](https://guides.library.oregonstate.edu/ld.php?content_id=45294345)  
  üìë [README template: Cornell](https://cornell.app.box.com/v/ReadmeTemplate)     
  üìë [README template: OSF](https://osf.io/sj8xv/)  
  üìë [README recommended content: UCI](https://guides.lib.uci.edu/datamanagement/readme)    
  üìë [README recommended content: Cornell](https://data.research.cornell.edu/content/readme)    
  üìë [README recommended content: IHEID](https://libguides.graduateinstitute.ch/rdm/readme)  

<br>

### üìì Metadata

Metadata is *data about data*. They are structured data that "provide information about the dataset to help people find, understand, and use your data" ([IES](https://ies.ed.gov/funding/datasharing_faq.asp))". Most of the time when you hear the term metadata, it will be referring to what I am going to call project-level metadata. Project-level metadata is a type of descriptive metadata that aids in finding your project through internet searches. It is like the "label of the dataset". At minimum the information captured should include who created the data, when the data were created, a title and description of the data, and a unique identifier for the data. This is the type of data we discussed earlier about capturing within the *General*, *File*, and *Access* portion of our README files. 

Common elements of project-level metadata include:

```{r, echo=FALSE, fig.align="center", out.width='80%'}

knitr::include_graphics("img/metadata.png")

```
Source: [University of Portland](https://libguides.up.edu/datamanagement/documentation)

Metadata can also also be categorized further into data-level and variable-level:

* Data-level metadata, which might be considered administrative metadata, focuses more on the details of each dataset such as instruments used to collect the data and software used to process the data. Lucikly, creating this metadata requires no extra effort because this is the exact information captured in sections like *Methodology* in your READMEs above.

* Variable-level metadata, which might be considered structural metadata, is data about the variables in your dataset. This can be both incorporated into your data, by adding attributes like variable and value labels, and it is also includes everything discussed earlier in your codebook and/or data dictionary. If your data is qualitative in nature (such as documents, photos, videos, or sound files), metadata such as date/time/location, tags/keywords, or measurement information can sometimes be embedded directly into your files ([IHEID](https://libguides.graduateinstitute.ch/rdm/qualitative)). 

The most notable thing about metadata is that many fields follow metadata standards that aid in the "retrieving and indexing" of your data ([University of Arizona](https://data.library.arizona.edu/best-practices/data-documentation-readme-metadata)). Standards include things such as agreed upon formats, vocabularies, and structure. For example social science often follows the Data Documentation Initiative ([DDI](https://ddialliance.org/)) standards. However, ([IES](https://ies.ed.gov/funding/datasharing_faq.asp)) states that education has no standards and that instead "researchers should document everything and strive to make notation as interpretable as possible". Metadata can either be embedded within data or included in a separate file such as a README.txt or .pdf file.

If you plan to archive your data, the data repository you plan to use may have standards in place that you will need to follow. Consult your repository's website for detailed information on what is required for archival. Many universities have archives that are available for use and IES also has recommended data repositories that can be found [here](https://ies.ed.gov/funding/datasharing_faq.asp).


Additional metadata information can be found here:  

  üìë [Oregon State](https://guides.library.oregonstate.edu/research-data-services/data-management-metadata#:~:text=Project%2Dlevel%20metadata%20describes%20the,Dataset%20title)  
  üìë [UCI](https://guides.lib.uci.edu/datamanagement/describe)    
  üìë [NCSU](https://www.lib.ncsu.edu/do/data-management/metadata)  
  üìë [ICPSR](https://www.icpsr.umich.edu/web/pages/datamanagement/dmp/plan.html)   
  üìë [DDI](https://ddialliance.org/training/getting-started/data-catalog)  
  üìë [ADS](https://guides.archaeologydataservice.ac.uk/g2gp/CreateData_1-2#ref-CreateData_1-2-9)  
  üìë [University of Helsinki](https://www.helsinki.fi/en/research/guide-for-data-documentation)  
  üìë [London School of Economics and Political Science](https://www.lse.ac.uk/library/research-support/research-data-management/metadata-and-documentation)  
  üìë [Washington University in St. Louis](https://libguides.wustl.edu/c.php?g=47355&p=305263)  
  üìë [Data Management for Researchers](https://pelagicpublishing.com/products/data-management-for-researchers-briney)  

<br>

### üìì Miscellaneous

Last you should consider documenting **all** your data processes. This may fit into one of the above mentioned documents, or it may be a separate text or markdown files (such as a README), a syntax file, or even an excel file. Keep this document in the folder related to the content and name it accordingly, Ex:`README_Name-of-process.txt`. Consider tracking things such as:

File origination information:

* For Project A, you received demographics files every spring. 
  + Every time you receive a file, document the name/location of the file, when it was received, and why
  + This is especially important when for example: 
    + That school district sends you a file, and then an updated version of that file 3 weeks later
      + When did you receive this? Why did you receive this? Were there errors in the previous documents?
* This kind of information is easy to track if you use versioning software such as Git or you use software that includes versioning such as SharePoint or Box because it allows you to comment each time you commit or save new files.

* Processes used to clean/create data (inputs and outputs) such as:

  + Syntax: `cleaning.R`  
  + Input: `district-data_raw_2020_09_08.xlsx`  
  + Output: `district-data_clean_2020_10_09.csv`   
<br>
* Processes to create tables and reports such as:
  + Step 1: Run the file `01_clean-the-data.R`  
  + Step 2: Run the file `02_check-errors.R`  
  + Step 3: Run the report `code 03_report.R`  
<br>
* And last, as mentioned above, if your team does not use a wiki, consider creating a README *style guide* in your main project folder where you document:
  + File naming conventions
  + File structure conventions
  + See **File Structure** in Training 2 for more information on these types of rules.

<br>
  
### üìì  Conclusion

The main takeaway is that it is important to document everything. Not only is it required by funders and by data repositories, as well as necessary for those submitting data requests, but it is crucial to the integrity and reproducibility of your project. While you should try to standardize your documentation as much as possible, don't get stuck in the details of the format or the structure. Instead focus on getting the content, rules, and procedures down before you forget.

Although, *publishing data* will come in a later module, I did want to wrap up by saying that when it comes time to publish your data (whether in a repository or data sharing through a PI), plan to include the following documentation with your final datasets:

* README file (for each dataset) - including the project-level metadata
* Data dictionaries and/or codebooks



  
